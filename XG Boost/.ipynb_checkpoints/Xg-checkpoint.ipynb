{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ed7a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "sb.set() # set the default Seaborn style for graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4bf5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_combined_data = pd.read_excel(\"Yearly Data.xlsx\", usecols=lambda x: 'Unnamed' not in x)\n",
    "combined_changed_data_together_overall = pd.read_excel(\"Yearly Change Data.xlsx\", usecols=lambda x: 'Unnamed' not in x)\n",
    "combined_changed_data_together_overall_percent = pd.read_excel(\"Yearly % Change Data.xlsx\", usecols=lambda x: 'Unnamed' not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27db64e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loadtxt\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import plot_tree\n",
    "from matplotlib.pylab import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe29a2",
   "metadata": {},
   "source": [
    "## XGBoost Default Parameters (Raw Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b89ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_combined_data_temp = annual_combined_data.iloc[1:,:].copy()\n",
    "annual_combined_data_temp.insert(1, live_birth_annual_change.columns[1], live_birth_annual_change.iloc[:, 1])\n",
    "\n",
    "temp_X = annual_combined_data_temp.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = annual_combined_data_temp[\"Change in Live Birth\"]\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"number of live_birth in the year\", 'Change in Live Birth'], axis = 1))\n",
    "\n",
    "all_accuracy = []\n",
    "\n",
    "for i in range(1):\n",
    "    # Split the Dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "print(\"Average over 1000 tries, Accuracy: %.2f%%\" % (sum(all_accuracy)/1000 * 100.0))\n",
    "\n",
    "# set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df5a17",
   "metadata": {},
   "source": [
    "## XGBoost Default Parameters (Change Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9559fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = annual_combined_data_temp[\"Change in Live Birth\"]\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "all_accuracy = []\n",
    "\n",
    "for i in range(1):\n",
    "    # Split the Dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "print(\"Average over 1000 tries, Accuracy: %.2f%%\" % (sum(all_accuracy)/1000 * 100.0))\n",
    "\n",
    "# set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30fc8b",
   "metadata": {},
   "source": [
    "## XGBoost Default Parameters (% Change Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba745ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall_percent.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = annual_combined_data_temp[\"Change in Live Birth\"]\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "all_accuracy = []\n",
    "\n",
    "for i in range(1):\n",
    "    # Split the Dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "print(\"Average over 1000 tries, Accuracy: %.2f%%\" % (sum(all_accuracy)/1000 * 100.0))\n",
    "\n",
    "# set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d541a0",
   "metadata": {},
   "source": [
    "## XGBoost Tuned Hyperparameters (Raw Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_combined_data_temp = annual_combined_data.iloc[1:,:].copy()\n",
    "annual_combined_data_temp.insert(1, live_birth_annual_change.columns[1], live_birth_annual_change.iloc[:, 1])\n",
    "\n",
    "temp_X = annual_combined_data_temp.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = annual_combined_data_temp[\"Change in Live Birth\"]\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"number of live_birth in the year\", 'Change in Live Birth'], axis = 1))\n",
    "\n",
    "all_accuracy = []\n",
    "\n",
    "for i in range(1):\n",
    "    # Split the Dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier(max_depth=3,gamma=0.2,eta=0.25)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    all_accuracy.append(accuracy)\n",
    "print(\"Average over 1000 tries, Accuracy: %.2f%%\" % (sum(all_accuracy)/1000 * 100.0))\n",
    "\n",
    "# set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4add4",
   "metadata": {},
   "source": [
    "## XGBoost Tuned Hyperparameters (Change Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a60aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = annual_combined_data_temp[\"Change in Live Birth\"]\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "all_accuracy = []\n",
    "\n",
    "for i in range(1):\n",
    "    # Split the Dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier(max_depth=3,gamma=0.2,eta=0.25)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "print(\"Average over 1000 tries, Accuracy: %.2f%%\" % (sum(all_accuracy)/1000 * 100.0))\n",
    "\n",
    "# set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31a73b",
   "metadata": {},
   "source": [
    "## XGBoost Tuned Hyperparameters (% Change Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall_percent.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = annual_combined_data_temp[\"Change in Live Birth\"]\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "all_accuracy = []\n",
    "\n",
    "for i in range(1):\n",
    "    # Split the Dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    # fit model no training data\n",
    "    model = XGBClassifier(max_depth=3,gamma=0.2,eta=0.25)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "print(\"Average over 1000 tries, Accuracy: %.2f%%\" % (sum(all_accuracy)/1000 * 100.0))\n",
    "\n",
    "# set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bf7a4",
   "metadata": {},
   "source": [
    "## XGBoost GridSearchCV (Raw Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_combined_data_temp = annual_combined_data.iloc[1:,:].copy()\n",
    "annual_combined_data_temp.insert(1, live_birth_annual_change.columns[1], live_birth_annual_change.iloc[:, 1])\n",
    "\n",
    "temp_X = annual_combined_data_temp.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = annual_combined_data_temp[\"Change in Live Birth\"]\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"number of live_birth in the year\", 'Change in Live Birth'], axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'max_depth': [3,5,6,9, None], \n",
    "         'n_estimators':[50, 70, 100, 150, 200],\n",
    "         'eta':[0.1,0.15,0.2,0.25,0.3],\n",
    "         'gamma':[0.1,0.2,0.3,0.4]}\n",
    "\n",
    "grid_model = GridSearchCV(XGBClassifier(), param_grid=param, cv=3)\n",
    "\n",
    "grid_model.fit(X_train,y_train)\n",
    "print(grid_model.best_score_)\n",
    "print(grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657de62b",
   "metadata": {},
   "source": [
    "## XGBoost GridSearchCV (Change Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall.copy()\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = combined_changed_data_together_overall_percent['Change in Live Birth']\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'max_depth': [3,5,6,9, None], \n",
    "         'n_estimators':[50, 70, 100, 150, 200],\n",
    "         'eta':[0.1,0.15,0.2,0.25,0.3],\n",
    "         'gamma':[0.1,0.2,0.3,0.4]}\n",
    "\n",
    "grid_model = GridSearchCV(XGBClassifier(), param_grid=param, cv=3)\n",
    "\n",
    "grid_model.fit(X_train,y_train)\n",
    "print(grid_model.best_score_)\n",
    "print(grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2c4b7",
   "metadata": {},
   "source": [
    "## XGBoost GridSearchCV (% Change Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681935d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall_percent.copy()\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = combined_changed_data_together_overall_percent['Change in Live Birth']\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'max_depth': [3,5,6,9, None], \n",
    "         'n_estimators':[50, 70, 100, 150, 200], \n",
    "         'eta':[0.1,0.15,0.2,0.25,0.3],\n",
    "         'gamma':[0.1,0.2,0.3,0.4]}\n",
    "\n",
    "grid_model = GridSearchCV(XGBClassifier(), param_grid=param, cv=3)\n",
    "\n",
    "grid_model.fit(X_train,y_train)\n",
    "print(grid_model.best_score_)\n",
    "print(grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2370f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = combined_changed_data_together_overall['Change in Live Birth']\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "import numpy as np\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier(max_depth=3, gamma=0.1, eta=0.1, n_estimators=70)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "##set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()\n",
    "\n",
    "import lime \n",
    "from lime import lime_tabular\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "shap_explainer = shap.TreeExplainer(model)\n",
    "shap_values = shap_explainer.shap_values(X)\n",
    "\n",
    "test_1 = X_test.iloc[3]\n",
    "\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values[:,:],\n",
    "    feature_names = X_train.columns,\n",
    "    class_names=['DECREASE','INCREASE'],\n",
    "    discretize_continuous=False,\n",
    "    verbose=True)\n",
    "i = 0\n",
    "j = 0\n",
    "while (i != 5):\n",
    "    if (X_test.iloc[j].isnull().values.any()):\n",
    "        j += 1\n",
    "        continue\n",
    "    print(f\"data row {j+1}\")\n",
    "    lime_exp = lime_explainer.explain_instance(\n",
    "        data_row=X_test.iloc[j,:],\n",
    "        predict_fn=model.predict_proba\n",
    "    )\n",
    "    lime_exp.show_in_notebook(show_table=True)\n",
    "    display(pd.DataFrame(lime_exp.as_list(),columns=['Feature','Contribution']))\n",
    "    print()\n",
    "    print()\n",
    "    display(shap.force_plot(shap_explainer.expected_value, shap_values[1, :], X_test.iloc[0,:]))\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    i += 1\n",
    "    j += 1\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39401eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall_percent.copy()\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = combined_changed_data_together_overall['Change in Live Birth']\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number'], axis = 1))\n",
    "\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier(max_depth=3, gamma=0.3, eta=0.25, n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "##set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()\n",
    "\n",
    "import lime \n",
    "from lime import lime_tabular\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "shap_explainer = shap.TreeExplainer(model)\n",
    "shap_values = shap_explainer.shap_values(X)\n",
    "\n",
    "test_1 = X_test.iloc[3]\n",
    "\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values[:,:],\n",
    "    feature_names = X_train.columns,\n",
    "    class_names=['DECREASE','INCREASE'],\n",
    "    discretize_continuous=False,\n",
    "    verbose=True)\n",
    "i = 0\n",
    "j = 0\n",
    "while (i != 5):\n",
    "    if (X_test.iloc[j].isnull().values.any()):\n",
    "        j += 1\n",
    "        continue\n",
    "    print(f\"data row {j+1}\")\n",
    "    lime_exp = lime_explainer.explain_instance(\n",
    "        data_row=X_test.iloc[j,:],\n",
    "        predict_fn=model.predict_proba\n",
    "    )\n",
    "    lime_exp.show_in_notebook(show_table=True)\n",
    "    display(pd.DataFrame(lime_exp.as_list(),columns=['Feature','Contribution']))\n",
    "    print()\n",
    "    print()\n",
    "    display(shap.force_plot(shap_explainer.expected_value, shap_values[1, :], X_test.iloc[0,:]))\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    i += 1\n",
    "    j += 1\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef644d8",
   "metadata": {},
   "source": [
    "## SHAP and LIME Feature Evaluation without Fertility Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf20299",
   "metadata": {},
   "source": [
    "### In our analysis, we calculated the SHAP values for each feature and visualized them using summary plots. The initial summary plot revealed that the fertility rate had a significant impact on the model for the annual datasets. Which is useful for predicting the change in the number of live births.\n",
    "\n",
    "### However, as it is merely a symptom of the fertility rate. The model was not useful in identifying the causes of the change in birth numbers.\n",
    "\n",
    "### As a result, we removed factors that could be a symptom of the birth number such as fertility rate and infant death from the model and ran it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall.copy()\n",
    "#temp_X.fillna(temp_X.mean(), inplace=True)\n",
    "#temp_X\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = combined_changed_data_together_overall['Change in Live Birth']\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number', 'Change in Fertility Rate'], axis = 1))\n",
    "\n",
    "#X.fillna(X.mean, inplace=True)\n",
    "#print(X)\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "#print(y_train)\n",
    "y_train = le.fit_transform(y_train)\n",
    "import numpy as np\n",
    "##print(type(y_train))\n",
    "\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier(max_depth=3, gamma=0.1, eta=0.1, n_estimators=70)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "##set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()\n",
    "\n",
    "import lime \n",
    "from lime import lime_tabular\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "shap_explainer = shap.TreeExplainer(model)\n",
    "shap_values = shap_explainer.shap_values(X)\n",
    "\n",
    "test_1 = X_test.iloc[3]\n",
    "\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values[:,:],\n",
    "    feature_names = X_train.columns,\n",
    "    class_names=['DECREASE','INCREASE'],\n",
    "    discretize_continuous=False,\n",
    "    verbose=True)\n",
    "i = 0\n",
    "j = 0\n",
    "while (i != 5):\n",
    "    if (X_test.iloc[j].isnull().values.any()):\n",
    "        j += 1\n",
    "        continue\n",
    "    print(f\"data row {j+1}\")\n",
    "    lime_exp = lime_explainer.explain_instance(\n",
    "        data_row=X_test.iloc[j,:],\n",
    "        predict_fn=model.predict_proba\n",
    "    )\n",
    "    lime_exp.show_in_notebook(show_table=True)\n",
    "    display(pd.DataFrame(lime_exp.as_list(),columns=['Feature','Contribution']))\n",
    "    print()\n",
    "    print()\n",
    "    display(shap.force_plot(shap_explainer.expected_value, shap_values[1, :], X_test.iloc[0,:]))\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    i += 1\n",
    "    j += 1\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65787a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = combined_changed_data_together_overall_percent.copy()\n",
    "#temp_X.fillna(temp_X.mean(), inplace=True)\n",
    "#temp_X\n",
    "\n",
    "temp_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Extract Response and Predictors\n",
    "y = combined_changed_data_together_overall['Change in Live Birth']\n",
    "X = pd.DataFrame(temp_X.drop(columns=[\"Year\", \"Change in Live Birth\", 'Change in Live Birth Number', 'Change in Fertility Rate'], axis = 1))\n",
    "\n",
    "#X.fillna(X.mean, inplace=True)\n",
    "#print(X)\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "#print(y_train)\n",
    "y_train = le.fit_transform(y_train)\n",
    "import numpy as np\n",
    "##print(type(y_train))\n",
    "\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier(max_depth=3, gamma=0.3, eta=0.25, n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "##set up the parameters\n",
    "rcParams['figure.figsize'] = 100,75\n",
    "plot_tree(model, rankdir=\"LR\")\n",
    "plt.title(\"Last Iteration's 1st Model\", fontsize = 70)\n",
    "plt.show()\n",
    "\n",
    "import lime \n",
    "from lime import lime_tabular\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "shap_explainer = shap.TreeExplainer(model)\n",
    "shap_values = shap_explainer.shap_values(X)\n",
    "\n",
    "test_1 = X_test.iloc[3]\n",
    "\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values[:,:],\n",
    "    feature_names = X_train.columns,\n",
    "    class_names=['DECREASE','INCREASE'],\n",
    "    discretize_continuous=False,\n",
    "    verbose=True)\n",
    "i = 0\n",
    "j = 0\n",
    "while (i != 5):\n",
    "    if (X_test.iloc[j].isnull().values.any()):\n",
    "        j += 1\n",
    "        continue\n",
    "    print(f\"data row {j+1}\")\n",
    "    lime_exp = lime_explainer.explain_instance(\n",
    "        data_row=X_test.iloc[j,:],\n",
    "        predict_fn=model.predict_proba\n",
    "    )\n",
    "    lime_exp.show_in_notebook(show_table=True)\n",
    "    display(pd.DataFrame(lime_exp.as_list(),columns=['Feature','Contribution']))\n",
    "    print()\n",
    "    print()\n",
    "    display(shap.force_plot(shap_explainer.expected_value, shap_values[1, :], X_test.iloc[0,:]))\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    i += 1\n",
    "    j += 1\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f67786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
